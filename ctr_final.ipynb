{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-QOi5M21NrQR"
      },
      "outputs": [],
      "source": [
        "# ==================================================================================\n",
        "# MM-CTR TASK 1&2: EMBEDDING GENERATION + CTR TRAINING + SUBMISSION\n",
        "# ==================================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import zipfile\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gJdirXENrN9",
        "outputId": "f705a997-e13e-47c5-ced5-96a386ea357d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing Dependencies...\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "print(\"üì¶ Installing Dependencies...\")\n",
        "\n",
        "try:\n",
        "    import clip\n",
        "except ImportError:\n",
        "    subprocess.check_call([\n",
        "        sys.executable, \"-m\", \"pip\", \"install\",\n",
        "        \"ftfy\", \"regex\", \"tqdm\", \"git+https://github.com/openai/CLIP.git\"\n",
        "    ])\n",
        "    import clip\n",
        "\n",
        "try:\n",
        "    import polars as pl\n",
        "except ImportError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"polars\"])\n",
        "    import polars as pl\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_W0HDHVNrLb",
        "outputId": "32b5045a-ef45-4c79-d1eb-cd147c5c7751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "class Config:\n",
        "    BASE_PATH = '/content/drive/MyDrive/compet/MicroLens_1M_MMCTR'  # UPDATE if needed\n",
        "    DATA_DIR = os.path.join(BASE_PATH, 'MicroLens_1M_x1')\n",
        "\n",
        "    FEATURE_PATH = os.path.join(BASE_PATH, 'item_feature.parquet')\n",
        "    RAR_PATH = os.path.join(BASE_PATH, 'item_images_2.rar')\n",
        "\n",
        "    GENERATED_EMB_PATH = os.path.join(BASE_PATH, 'item_emb_task1_clip.parquet')\n",
        "\n",
        "    MODEL_SAVE_DIR = os.path.join(BASE_PATH, 'models_task1and2')\n",
        "    PRED_SAVE_DIR = os.path.join(BASE_PATH, 'predictions_task1and2')\n",
        "\n",
        "    IMG_EXTRACT_PATH = '/content/item_images'\n",
        "\n",
        "    BATCH_SIZE_CLIP = 128\n",
        "    BATCH_SIZE_TRAIN = 2048\n",
        "    EMBED_DIM = 128\n",
        "    SIDE_EMBED_DIM = 16\n",
        "    LR = 5e-4\n",
        "    EPOCHS = 30\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "config = Config()\n",
        "os.makedirs(config.MODEL_SAVE_DIR, exist_ok=True)\n",
        "os.makedirs(config.PRED_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Device:\", config.DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ue3rpQHUNrJL"
      },
      "outputs": [],
      "source": [
        "def setup_images():\n",
        "    if not os.path.exists(config.IMG_EXTRACT_PATH):\n",
        "        print(f\"üìÇ Extracting images from {config.RAR_PATH}...\")\n",
        "        subprocess.run(\"apt-get update -y\", shell=True, check=False)\n",
        "        subprocess.run(\"apt-get install -y unrar\", shell=True, check=False)\n",
        "\n",
        "        os.makedirs(config.IMG_EXTRACT_PATH, exist_ok=True)\n",
        "        cmd = f\"unrar x -inul '{config.RAR_PATH}' '{config.IMG_EXTRACT_PATH}/'\"\n",
        "        subprocess.run(cmd, shell=True, check=False)\n",
        "        print(\"‚úÖ Images extracted.\")\n",
        "    else:\n",
        "        print(\"‚úÖ Images already ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mVAJEdJXcMw7"
      },
      "outputs": [],
      "source": [
        "class CLIPDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess):\n",
        "        self.item_ids = df['item_id'].values\n",
        "        self.titles = df['item_title'].fillna(\"\").astype(str).values\n",
        "        self.img_dir = img_dir\n",
        "        if os.path.exists(os.path.join(img_dir, 'item_images')):\n",
        "            self.img_dir = os.path.join(img_dir, 'item_images')\n",
        "        self.preprocess = preprocess\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.item_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item_id = self.item_ids[idx]\n",
        "\n",
        "        text = str(self.titles[idx])[:77]\n",
        "        text_tensor = clip.tokenize([text], truncate=True).squeeze(0)\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, f\"{item_id}.jpg\")\n",
        "        if os.path.exists(img_path):\n",
        "            try:\n",
        "                image = Image.open(img_path).convert(\"RGB\")\n",
        "                img_tensor = self.preprocess(image)\n",
        "            except Exception:\n",
        "                img_tensor = torch.zeros(3, 224, 224)\n",
        "        else:\n",
        "            img_tensor = torch.zeros(3, 224, 224)\n",
        "\n",
        "        return item_id, text_tensor, img_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqUn6dMCcMqU",
        "outputId": "663a2301-6a1b-4bbc-e3a7-4a99ea5b6250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- STEP 1: GENERATING EMBEDDINGS ---\n",
            "‚úÖ Found existing embeddings at /content/drive/MyDrive/compet/MicroLens_1M_MMCTR/item_emb_task1_clip.parquet. Skipping generation.\n"
          ]
        }
      ],
      "source": [
        "def generate_embeddings():\n",
        "    print(\"\\n--- STEP 1: GENERATING EMBEDDINGS ---\")\n",
        "\n",
        "    if os.path.exists(config.GENERATED_EMB_PATH):\n",
        "        print(f\"‚úÖ Found existing embeddings at {config.GENERATED_EMB_PATH}. Skipping generation.\")\n",
        "        return\n",
        "\n",
        "    setup_images()\n",
        "\n",
        "    print(\"üß† Loading CLIP...\")\n",
        "    model, preprocess = clip.load(\"ViT-B/32\", device=config.DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    print(\"üìÑ Reading item features...\")\n",
        "    df = pd.read_parquet(config.FEATURE_PATH)\n",
        "\n",
        "    ds = CLIPDataset(df, config.IMG_EXTRACT_PATH, preprocess)\n",
        "    dl = DataLoader(ds, batch_size=config.BATCH_SIZE_CLIP, shuffle=False, num_workers=2)\n",
        "\n",
        "    all_emb = []\n",
        "    all_ids = []\n",
        "\n",
        "    print(\"‚ö° Extracting CLIP embeddings...\")\n",
        "    with torch.no_grad():\n",
        "        for ids, text, imgs in tqdm(dl):\n",
        "            text = text.to(config.DEVICE)\n",
        "            imgs = imgs.to(config.DEVICE)\n",
        "\n",
        "            txt_feat = model.encode_text(text)\n",
        "            img_feat = model.encode_image(imgs)\n",
        "\n",
        "            txt_feat = txt_feat / (txt_feat.norm(dim=-1, keepdim=True) + 1e-12)\n",
        "            img_feat = img_feat / (img_feat.norm(dim=-1, keepdim=True) + 1e-12)\n",
        "\n",
        "            combined = (txt_feat + img_feat) / 2.0\n",
        "            all_emb.append(combined.detach().cpu().numpy())\n",
        "            all_ids.extend(ids.numpy().tolist())\n",
        "\n",
        "    raw_matrix = np.vstack(all_emb)\n",
        "\n",
        "    del model, text, imgs, txt_feat, img_feat\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"üìâ Reducing Dimensions ({raw_matrix.shape} -> 128)...\")\n",
        "    pca = PCA(n_components=128, random_state=42)\n",
        "    reduced_matrix = pca.fit_transform(raw_matrix)\n",
        "    print(f\"   Explained Variance: {float(np.sum(pca.explained_variance_ratio_)):.2f}\")\n",
        "\n",
        "    df_out = pd.DataFrame({\n",
        "        'item_id': all_ids,\n",
        "        'item_emb_d128': list(reduced_matrix.astype(np.float32))\n",
        "    })\n",
        "    df_out.to_parquet(config.GENERATED_EMB_PATH, index=False)\n",
        "    print(f\"üíæ Saved embeddings: {config.GENERATED_EMB_PATH}\")\n",
        "\n",
        "    del raw_matrix, reduced_matrix, all_emb, df, df_out\n",
        "    gc.collect()\n",
        "\n",
        "generate_embeddings()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OcZMubrmcMmi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29851651-0798-4f3f-ce0b-9902f1b9438c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- STEP 2: PREPARING TRAINING ASSETS ---\n",
            "üõ†Ô∏è  Loading embeddings from: /content/drive/MyDrive/compet/MicroLens_1M_MMCTR/item_emb_task1_clip.parquet\n",
            "‚úÖ Matrix shape: (91718, 128) (padding included)\n"
          ]
        }
      ],
      "source": [
        "def load_assets_task1and2():\n",
        "    print(\"\\n--- STEP 2: PREPARING TRAINING ASSETS ---\")\n",
        "    print(f\"üõ†Ô∏è  Loading embeddings from: {config.GENERATED_EMB_PATH}\")\n",
        "\n",
        "    df_emb = pl.read_parquet(config.GENERATED_EMB_PATH)\n",
        "\n",
        "    real_ids = df_emb['item_id'].to_list()\n",
        "    id_to_idx = {rid: i + 1 for i, rid in enumerate(real_ids)}\n",
        "\n",
        "    vectors = np.array(df_emb['item_emb_d128'].to_list(), dtype=np.float32)\n",
        "    padding = np.zeros((1, config.EMBED_DIM), dtype=np.float32)\n",
        "    matrix = np.vstack([padding, vectors])\n",
        "\n",
        "    print(f\"‚úÖ Matrix shape: {matrix.shape} (padding included)\")\n",
        "    del df_emb, vectors\n",
        "    gc.collect()\n",
        "\n",
        "    return torch.tensor(matrix), id_to_idx\n",
        "\n",
        "PRETRAINED_WEIGHTS, ID_MAP = load_assets_task1and2()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SxP6BVLccQax"
      },
      "outputs": [],
      "source": [
        "class RichDataset(Dataset):\n",
        "    def __init__(self, parquet_path, id_map, is_test=False):\n",
        "        df = pl.read_parquet(parquet_path)\n",
        "\n",
        "        def map_ids(arr):\n",
        "            return np.array([id_map.get(x, 0) for x in arr], dtype=np.int32)\n",
        "\n",
        "        self.target = map_ids(df['item_id'].to_numpy())\n",
        "        seq_matrix = np.stack(df['item_seq'].to_numpy())\n",
        "        self.history = map_ids(seq_matrix.flatten()).reshape(seq_matrix.shape)\n",
        "\n",
        "        self.likes = df['likes_level'].to_numpy().astype(np.int32)\n",
        "        self.views = df['views_level'].to_numpy().astype(np.int32)\n",
        "\n",
        "        if not is_test:\n",
        "            self.label = df['label'].to_numpy().astype(np.float32)\n",
        "            self.ids = None\n",
        "        else:\n",
        "            self.label = np.zeros(len(df), dtype=np.float32)\n",
        "            self.ids = df['ID'].to_numpy().astype(np.int32)\n",
        "\n",
        "        del df, seq_matrix\n",
        "        gc.collect()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            self.history[idx],\n",
        "            self.target[idx],\n",
        "            self.likes[idx],\n",
        "            self.views[idx],\n",
        "            self.label[idx]\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gipzRFQicQYS"
      },
      "outputs": [],
      "source": [
        "class Dice(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm1d(num_features, eps=1e-9)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.alpha = nn.Parameter(torch.zeros((num_features,)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        p = self.sig(self.bn(x))\n",
        "        return p * x + (1 - p) * self.alpha * x\n",
        "\n",
        "\n",
        "class DIN_Task1and2(nn.Module):\n",
        "    def __init__(self, weights):\n",
        "        super().__init__()\n",
        "        num_items, dim = weights.shape\n",
        "\n",
        "        self.item_emb = nn.Embedding(num_items, dim, padding_idx=0)\n",
        "        self.item_emb.weight.data.copy_(weights)\n",
        "        self.item_emb.weight.requires_grad = True\n",
        "\n",
        "        self.likes_emb = nn.Embedding(20, config.SIDE_EMBED_DIM)\n",
        "        self.views_emb = nn.Embedding(20, config.SIDE_EMBED_DIM)\n",
        "\n",
        "        self.att_mlp = nn.Sequential(\n",
        "            nn.Linear(dim * 4, 80), nn.Sigmoid(),\n",
        "            nn.Linear(80, 40), nn.Sigmoid(),\n",
        "            nn.Linear(40, 1)\n",
        "        )\n",
        "\n",
        "        in_dim = dim * 2 + config.SIDE_EMBED_DIM * 2\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, 512), Dice(512), nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256), Dice(256), nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def attention(self, target, history, mask):\n",
        "        seq_len = history.size(1)\n",
        "        target_tile = target.expand(-1, seq_len, -1)\n",
        "        inp = torch.cat([target_tile, history, target_tile - history, target_tile * history], dim=-1)\n",
        "        scores = self.att_mlp(inp).masked_fill(mask.unsqueeze(-1) == 0, -1e9)\n",
        "        weighted = (torch.softmax(scores, dim=1) * history).sum(dim=1)\n",
        "        return weighted\n",
        "\n",
        "    def forward(self, history, target, likes, views):\n",
        "        h_emb = self.item_emb(history)\n",
        "        t_emb = self.item_emb(target).unsqueeze(1)\n",
        "\n",
        "        mask = (history != 0)\n",
        "        user_int = self.attention(t_emb, h_emb, mask)\n",
        "\n",
        "        feats = torch.cat([\n",
        "            t_emb.squeeze(1),\n",
        "            user_int,\n",
        "            self.likes_emb(likes),\n",
        "            self.views_emb(views)\n",
        "        ], dim=1)\n",
        "\n",
        "        return self.mlp(feats).squeeze(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b99nd9pLcTwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b09dfca-6319-48a8-cec5-51b5e2a25d3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- STEP 3: TRAINING CTR MODEL (Task1&2) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1758/1758 [01:11<00:00, 24.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Epoch 1: Loss=0.2158 | Val AUC=0.8756 | LR=5.0e-04\n",
            "üèÜ New best model saved: AUC=0.8756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1758/1758 [01:09<00:00, 25.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Epoch 2: Loss=0.0980 | Val AUC=0.9280 | LR=5.0e-04\n",
            "üèÜ New best model saved: AUC=0.9280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1758/1758 [01:09<00:00, 25.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Epoch 3: Loss=0.0833 | Val AUC=0.9384 | LR=5.0e-04\n",
            "üèÜ New best model saved: AUC=0.9384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1758/1758 [01:09<00:00, 25.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Epoch 4: Loss=0.0761 | Val AUC=0.9461 | LR=5.0e-04\n",
            "üèÜ New best model saved: AUC=0.9461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1758/1758 [01:09<00:00, 25.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Epoch 5: Loss=0.0689 | Val AUC=0.9494 | LR=5.0e-04\n",
            "üèÜ New best model saved: AUC=0.9494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1758/1758 [01:09<00:00, 25.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Epoch 6: Loss=0.0636 | Val AUC=0.9481 | LR=5.0e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1758/1758 [01:09<00:00, 25.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Epoch 7: Loss=0.0607 | Val AUC=0.9491 | LR=5.0e-04\n",
            "üîª LR reduced to 2.5e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1758/1758 [01:09<00:00, 25.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Epoch 8: Loss=0.0585 | Val AUC=0.9526 | LR=2.5e-04\n",
            "üèÜ New best model saved: AUC=0.9526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1758/1758 [01:10<00:00, 24.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Epoch 9: Loss=0.0576 | Val AUC=0.9536 | LR=2.5e-04\n",
            "üèÜ New best model saved: AUC=0.9536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1758/1758 [01:09<00:00, 25.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Epoch 10: Loss=0.0573 | Val AUC=0.9542 | LR=2.5e-04\n",
            "üèÜ New best model saved: AUC=0.9542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1758/1758 [01:09<00:00, 25.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Epoch 11: Loss=0.0572 | Val AUC=0.9554 | LR=2.5e-04\n",
            "üèÜ New best model saved: AUC=0.9554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1758/1758 [01:09<00:00, 25.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Epoch 12: Loss=0.0571 | Val AUC=0.9546 | LR=2.5e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1758/1758 [01:09<00:00, 25.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Epoch 13: Loss=0.0570 | Val AUC=0.9540 | LR=2.5e-04\n",
            "üîª LR reduced to 1.3e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1758/1758 [01:09<00:00, 25.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Epoch 14: Loss=0.0568 | Val AUC=0.9550 | LR=1.3e-04\n",
            "‚õî Early stopping: no improvement for 3 epochs.\n",
            "\n",
            "‚úÖ Best Val AUC: 0.9554\n"
          ]
        }
      ],
      "source": [
        "def train_model():\n",
        "    print(\"\\n--- STEP 3: TRAINING CTR MODEL (Task1&2) ---\")\n",
        "\n",
        "    train_path = os.path.join(config.DATA_DIR, 'train.parquet')\n",
        "    valid_path = os.path.join(config.DATA_DIR, 'valid.parquet')\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        RichDataset(train_path, ID_MAP),\n",
        "        batch_size=config.BATCH_SIZE_TRAIN,\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    valid_dl = DataLoader(\n",
        "        RichDataset(valid_path, ID_MAP),\n",
        "        batch_size=config.BATCH_SIZE_TRAIN * 2,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    model = DIN_Task1and2(PRETRAINED_WEIGHTS).to(config.DEVICE)\n",
        "\n",
        "    # ‚úÖ Regularisation + stabilit√©\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=config.LR, weight_decay=1e-4)  # ‚úÖ weight_decay ajout√©\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "      optimizer, mode='max', factor=0.5, patience=1\n",
        "    )\n",
        "\n",
        "\n",
        "    best_auc = 0.0\n",
        "    best_path = os.path.join(config.MODEL_SAVE_DIR, 'task1and2_best.pt')\n",
        "\n",
        "    # ‚úÖ Early stopping\n",
        "    patience = 3\n",
        "    no_improve = 0\n",
        "    min_delta = 1e-4\n",
        "\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for hist, tgt, lk, vw, lbl in tqdm(train_dl, desc=f\"Epoch {epoch+1}/{config.EPOCHS}\"):\n",
        "            hist = hist.to(config.DEVICE).long()\n",
        "            tgt = tgt.to(config.DEVICE).long()\n",
        "            lk = lk.to(config.DEVICE).long()\n",
        "            vw = vw.to(config.DEVICE).long()\n",
        "            lbl = lbl.to(config.DEVICE).float()\n",
        "\n",
        "            # ‚úÖ Label smoothing l√©ger (option safe)\n",
        "            lbl = lbl * 0.98 + 0.01\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(hist, tgt, lk, vw)\n",
        "            loss = criterion(logits, lbl)\n",
        "            loss.backward()\n",
        "\n",
        "            # ‚úÖ Gradient clipping pour stabilit√©\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += float(loss.item())\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        preds, labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for hist, tgt, lk, vw, lbl in valid_dl:\n",
        "                hist = hist.to(config.DEVICE).long()\n",
        "                tgt = tgt.to(config.DEVICE).long()\n",
        "                lk = lk.to(config.DEVICE).long()\n",
        "                vw = vw.to(config.DEVICE).long()\n",
        "\n",
        "                logits = model(hist, tgt, lk, vw)\n",
        "                preds.extend(torch.sigmoid(logits).cpu().numpy().tolist())\n",
        "                labels.extend(lbl.numpy().tolist())\n",
        "\n",
        "        auc = roc_auc_score(labels, preds)\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"üìä Epoch {epoch+1}: Loss={total_loss/len(train_dl):.4f} | Val AUC={auc:.4f} | LR={lr:.1e}\")\n",
        "\n",
        "        scheduler.step(auc)\n",
        "        new_lr = optimizer.param_groups[0]['lr']\n",
        "        if new_lr != lr:\n",
        "            print(f\"üîª LR reduced to {new_lr:.1e}\")\n",
        "\n",
        "\n",
        "        # ‚úÖ Save best + early stop\n",
        "        if auc > best_auc + min_delta:\n",
        "            best_auc = auc\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            print(f\"üèÜ New best model saved: AUC={auc:.4f}\")\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(f\"‚õî Early stopping: no improvement for {patience} epochs.\")\n",
        "                break\n",
        "\n",
        "    print(f\"\\n‚úÖ Best Val AUC: {best_auc:.4f}\")\n",
        "    return best_path\n",
        "\n",
        "best_model_path = train_model()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FMndgfmdcTjR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "4e466501-8dfa-4996-aa84-21b415fe100d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- STEP 4: PREDICT + SUBMISSION (Task1&2) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 93/93 [00:02<00:00, 31.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Submission ready: /content/drive/MyDrive/compet/MicroLens_1M_MMCTR/predictions_task1and2/prediction.zip\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/compet/MicroLens_1M_MMCTR/predictions_task1and2/prediction.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def predict_and_submit(best_path):\n",
        "    print(\"\\n--- STEP 4: PREDICT + SUBMISSION (Task1&2) ---\")\n",
        "\n",
        "    test_path = os.path.join(config.DATA_DIR, 'test.parquet')\n",
        "\n",
        "    test_ds = RichDataset(test_path, ID_MAP, is_test=True)\n",
        "    test_dl = DataLoader(\n",
        "        test_ds,\n",
        "        batch_size=config.BATCH_SIZE_TRAIN * 2,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    model = DIN_Task1and2(PRETRAINED_WEIGHTS).to(config.DEVICE)\n",
        "    model.load_state_dict(torch.load(best_path, map_location=config.DEVICE))\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    with torch.no_grad():\n",
        "        for hist, tgt, lk, vw, _ in tqdm(test_dl):\n",
        "            hist = hist.to(config.DEVICE).long()\n",
        "            tgt = tgt.to(config.DEVICE).long()\n",
        "            lk = lk.to(config.DEVICE).long()\n",
        "            vw = vw.to(config.DEVICE).long()\n",
        "\n",
        "            logits = model(hist, tgt, lk, vw)\n",
        "            all_preds.extend(torch.sigmoid(logits).cpu().numpy().tolist())\n",
        "\n",
        "    # ‚úÖ CORRECTION TASK1&2: fill Task1&2 column, not Task1\n",
        "    df = pd.DataFrame({\n",
        "        'ID': test_ds.ids,\n",
        "        'Task1': 0,\n",
        "        'Task2': 0,\n",
        "        'Task1&2': all_preds\n",
        "    })\n",
        "\n",
        "    csv_path = os.path.join(config.PRED_SAVE_DIR, 'prediction.csv')\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    zip_path = os.path.join(config.PRED_SAVE_DIR, 'prediction.zip')\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:\n",
        "        z.write(csv_path, 'prediction.csv')\n",
        "\n",
        "    print(f\"‚úÖ Submission ready: {zip_path}\")\n",
        "    return zip_path\n",
        "\n",
        "zip_path = predict_and_submit(best_model_path)\n",
        "zip_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3ZhAhjnUcTeC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}